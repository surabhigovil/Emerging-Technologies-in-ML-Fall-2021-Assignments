{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djImDyBVI6Kc",
        "outputId": "199c2f48-31a5-4575-db48-03e95eb3740f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in c:\\users\\abhin\\anaconda3\\envs\\torch\\lib\\site-packages (0.14.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in c:\\users\\abhin\\anaconda3\\envs\\torch\\lib\\site-packages (from tensorflow-addons) (2.12.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uqz3qRlNJJX8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "\n",
        "import os, sys\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as _Imgdis\n",
        "from PIL import Image\n",
        "from time import time\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcF0q3PK81kG"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pqj5dW1m-GW",
        "outputId": "f936d5e9-7943-43a5-f2ce-011273bb9513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 27s 0us/step\n",
            "170508288/170498071 [==============================] - 27s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2JQ0P2_ndrU",
        "outputId": "f033d82b-43ba-4de6-e4d4-a79f5e9d904f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "input_shape = (32,32,3)\n",
        "\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VOf05h2JzYe",
        "outputId": "d4ebb355-06a4-429d-a659-2bf0079395e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image size: 32 X 32 = 1024\n",
            "Patch size: 2 X 2 = 4 \n",
            "Patches per image: 256\n",
            "Elements per patch (3 channel): 12\n",
            "Latent array shape: 256 X 256\n",
            "Data array shape: 256 X 256\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 8\n",
        "num_epochs = 5\n",
        "dropout_rate = 0.2\n",
        "image_size = 32  # We'll resize input images to this size.\n",
        "patch_size = 2  # Size of the patches to be extract from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "latent_dim = 256  # Size of the latent array.\n",
        "projection_dim = 256  # Embedding size of each element in the data and latent arrays.\n",
        "num_heads = 8  # Number of Transformer heads.\n",
        "ffn_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]  # Size of the Transformer Feedforward network.\n",
        "num_transformer_blocks = 4\n",
        "num_iterations = 2  # Repetitions of the cross-attention and Transformer modules.\n",
        "classifier_units = [\n",
        "    projection_dim,\n",
        "    num_classes,\n",
        "]  # Size of the Feedforward network of the final classifier.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channel): {(patch_size ** 2) * 3}\")\n",
        "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
        "print(f\"Data array shape: {num_patches} X {projection_dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBqzrzRBOux"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "M79fFJajJ1yn"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beHmMzBeBLYi"
      },
      "source": [
        "# Feed Forward Network Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Rn1UvglcKCi_"
      },
      "outputs": [],
      "source": [
        "def create_ffn(hidden_units, dropout_rate):\n",
        "    ffn_layers = []\n",
        "    for units in hidden_units[:-1]:\n",
        "        ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n",
        "    ffn_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    ffn = keras.Sequential(ffn_layers)\n",
        "    return ffn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_82-zr0GCGxv"
      },
      "source": [
        "# Creating Patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DZ3we3XmKFFv"
      },
      "outputs": [],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jkuIYRcdKHda"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patches):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvLMZ87MCJRO"
      },
      "source": [
        "# Defining Perciever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seVbclrICLiE"
      },
      "source": [
        "## Cross Attention "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "12K9oqySKJ0E"
      },
      "outputs": [],
      "source": [
        "def create_cross_attention_module(\n",
        "    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate\n",
        "):\n",
        "\n",
        "    inputs = {\n",
        "        # Recieve the latent array as an input of shape [1, latent_dim, projection_dim].\n",
        "        \"latent_array\": layers.Input(shape=(latent_dim, projection_dim)),\n",
        "        # Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].\n",
        "        \"data_array\": layers.Input(shape=(data_dim, projection_dim)),\n",
        "    }\n",
        "\n",
        "    # Apply layer norm to the inputs\n",
        "    latent_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"latent_array\"])\n",
        "    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n",
        "\n",
        "    # Create query tensor: [1, latent_dim, projection_dim].\n",
        "    query = layers.Dense(units=projection_dim)(latent_array)\n",
        "    # Create key tensor: [batch_size, data_dim, projection_dim].\n",
        "    key = layers.Dense(units=projection_dim)(data_array)\n",
        "    # Create value tensor: [batch_size, data_dim, projection_dim].\n",
        "    value = layers.Dense(units=projection_dim)(data_array)\n",
        "\n",
        "    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
        "    attention_output = layers.Attention(use_scale=True, dropout=0.1)(\n",
        "        [query, key, value], return_attention_scores=False\n",
        "    )\n",
        "    # Skip connection 1.\n",
        "    attention_output = layers.Add()([attention_output, latent_array])\n",
        "\n",
        "    # Apply layer norm.\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "    # Apply Feedforward network.\n",
        "    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "    outputs = ffn(attention_output)\n",
        "    # Skip connection 2.\n",
        "    outputs = layers.Add()([outputs, attention_output])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8yHVFUiCXYA"
      },
      "source": [
        "## Transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FM-q72CHKMln"
      },
      "outputs": [],
      "source": [
        "def create_transformer_module(\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "):\n",
        "\n",
        "    # input_shape: [1, latent_dim, projection_dim]\n",
        "    inputs = layers.Input(shape=(latent_dim, projection_dim))\n",
        "\n",
        "    x0 = inputs\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        # Apply layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n",
        "        # Create a multi-head self-attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x0])\n",
        "        # Apply layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # Apply Feedforward network.\n",
        "        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "        x3 = ffn(x3)\n",
        "        # Skip connection 2.\n",
        "        x0 = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=x0)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzRlXIwOCf6q"
      },
      "source": [
        "# Perciever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PelrsDaeKOuf"
      },
      "outputs": [],
      "source": [
        "class Perceiver(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size,\n",
        "        data_dim,\n",
        "        latent_dim,\n",
        "        projection_dim,\n",
        "        num_heads,\n",
        "        num_transformer_blocks,\n",
        "        ffn_units,\n",
        "        dropout_rate,\n",
        "        num_iterations,\n",
        "        classifier_units,\n",
        "    ):\n",
        "        super(Perceiver, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.ffn_units = ffn_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.classifier_units = classifier_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create latent array.\n",
        "        self.latent_array = self.add_weight(\n",
        "            shape=(self.latent_dim, self.projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Create patching module.\n",
        "        self.patcher = Patches(self.patch_size)\n",
        "\n",
        "        # Create patch encoder.\n",
        "        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n",
        "\n",
        "        # Create cross-attenion module.\n",
        "        self.cross_attention = create_cross_attention_module(\n",
        "            self.latent_dim,\n",
        "            self.data_dim,\n",
        "            self.projection_dim,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create Transformer module.\n",
        "        self.transformer = create_transformer_module(\n",
        "            self.latent_dim,\n",
        "            self.projection_dim,\n",
        "            self.num_heads,\n",
        "            self.num_transformer_blocks,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create global average pooling layer.\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "\n",
        "        # Create a classification head.\n",
        "        self.classification_head = create_ffn(\n",
        "            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n",
        "        )\n",
        "\n",
        "        super(Perceiver, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Augment data.\n",
        "        augmented = data_augmentation(inputs)\n",
        "        # Create patches.\n",
        "        patches = self.patcher(augmented)\n",
        "        # Encode patches.\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "        # Prepare cross-attention inputs.\n",
        "        cross_attention_inputs = {\n",
        "            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n",
        "            \"data_array\": encoded_patches,\n",
        "        }\n",
        "        # Apply the cross-attention and the Transformer modules iteratively.\n",
        "        for _ in range(self.num_iterations):\n",
        "            # Apply cross-attention from the latent array to the data array.\n",
        "            latent_array = self.cross_attention(cross_attention_inputs)\n",
        "            # Apply self-attention Transformer to the latent array.\n",
        "            latent_array = self.transformer(latent_array)\n",
        "            # Set the latent array of the next iteration.\n",
        "            cross_attention_inputs[\"latent_array\"] = latent_array\n",
        "\n",
        "        # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.\n",
        "        representation = self.global_average_pooling(latent_array)\n",
        "        # Generate logits.\n",
        "        logits = self.classification_head(representation)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fenzvRtBCl4q"
      },
      "source": [
        "# Compiling, Training and Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9e_FTxxDKRin"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "\n",
        "    # Create LAMB optimizer with weight decay.\n",
        "    optimizer = tfa.optimizers.LAMB(\n",
        "        learning_rate=learning_rate, weight_decay_rate=weight_decay,\n",
        "    )\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.2, patience=3\n",
        "    )\n",
        "\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gss3sbCOKTqn",
        "outputId": "f54d2dcf-cfc7-41f7-b9c5-ee95117d5d8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "5625/5625 [==============================] - 15795s 3s/step - loss: 2.0864 - acc: 0.2217 - top5-acc: 0.7740 - val_loss: 2.0158 - val_acc: 0.2394 - val_top5-acc: 0.7770\n",
            "Epoch 2/5\n",
            "5625/5625 [==============================] - 15330s 3s/step - loss: 2.0551 - acc: 0.2301 - top5-acc: 0.7856 - val_loss: 1.9828 - val_acc: 0.2368 - val_top5-acc: 0.7978\n",
            "Epoch 3/5\n",
            "5625/5625 [==============================] - 15297s 3s/step - loss: 2.0819 - acc: 0.2178 - top5-acc: 0.7770 - val_loss: 2.0444 - val_acc: 0.2100 - val_top5-acc: 0.7654\n",
            "Epoch 4/5\n",
            "5625/5625 [==============================] - 15244s 3s/step - loss: 2.0753 - acc: 0.2110 - top5-acc: 0.7792 - val_loss: 1.9539 - val_acc: 0.2524 - val_top5-acc: 0.8164\n",
            "Epoch 5/5\n",
            "5625/5625 [==============================] - 15281s 3s/step - loss: 2.0752 - acc: 0.2113 - top5-acc: 0.7795 - val_loss: 2.0482 - val_acc: 0.2168 - val_top5-acc: 0.7672\n",
            "313/313 [==============================] - 1044s 3s/step - loss: 2.0480 - acc: 0.2158 - top5-acc: 0.7665\n",
            "Test accuracy: 21.58%\n",
            "Test top 5 accuracy: 76.65%\n"
          ]
        }
      ],
      "source": [
        "perceiver_classifier = Perceiver(\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "    num_iterations,\n",
        "    classifier_units,\n",
        ")\n",
        "\n",
        "\n",
        "history = run_experiment(perceiver_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "xI57zzQvfh1G",
        "outputId": "9f0196ec-b899-4cdb-d7f2-188710710b2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x19e3e6a3490>]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# visualizing losses and accuracy\n",
        "train_loss = history.history['loss']\n",
        "val_loss   = history.history['val_loss']\n",
        "train_acc  = history.history['acc']\n",
        "val_acc    = history.history['val_acc']\n",
        "xc         = range(num_epochs)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xc, train_loss)\n",
        "plt.plot(xc, val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GASU5ffv6Vg5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "HW2_Part(a)_Perciever.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
